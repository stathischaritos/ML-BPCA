{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA\n",
      "\n",
      "### Machine Learning: Principles and Methods, November 2013\n",
      "\n",
      "* The lab exercises should be made in groups of three people, or at least two people.\n",
      "* The deadline is Friday, 13 December, 23:59.\n",
      "* Assignment should be sent to T.S.Cohen at uva dot nl (Taco Cohen). The subject line of your email should be \"[MLPM2013] lab#_lastname1\\_lastname2\\_lastname3\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab01\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "import scipy.special as special\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        q = d\n",
      "        self.q = q\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(q, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(q, q)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, q)\n",
      "        self.sigma_w = np.random.randn(q, q)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(q, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        E_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        E_W = self.means_w\n",
      "        E_mu = self.mean_mu\n",
      "        for n in range(0,self.N):\n",
      "            t_n = np.reshape(X.T[n],(self.d,1)) \n",
      "            self.means_z.T[n] = reshape(E_tau * dot ( dot( self.sigma_z , E_W.T ) , (t_n - E_mu)) ,self.q)\n",
      "        self.sigma_z = np.linalg.inv(np.identity(self.q) + E_tau*(dot(E_W.T,E_W)))\n",
      "        \n",
      "    def __update_mu(self,X):\n",
      "        E_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        E_W = self.means_w\n",
      "        S = 0\n",
      "        for n in range(0,self.N):\n",
      "            t_n = np.reshape(X.T[n],(self.d,1)) \n",
      "            x_n = np.reshape(self.means_z.T[n],(self.q,1)) \n",
      "            S += t_n - dot(E_W,x_n)\n",
      "        self.mean_mu = E_tau*dot(self.sigma_mu,S)\n",
      "        self.sigma_mu = np.identity(self.d) / (self.beta + self.N*(self.a_tau_tilde/self.b_tau_tilde))\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        \n",
      "        S = 0\n",
      "        for n in range(0,self.N):\n",
      "            x_n = np.reshape(self.means_z.T[n],(self.q,1)) \n",
      "            S += self.sigma_z + dot(x_n,x_n.T)\n",
      "        self.sigma_w = np.linalg.inv( diagflat(self.a_alpha_tilde/self.bs_alpha_tilde) + (self.a_tau_tilde/self.b_tau_tilde)*(S))\n",
      "        \n",
      "        \n",
      "        E_tau = self.a_tau_tilde / self.b_tau_tilde\n",
      "        S = 0\n",
      "        for k in range(0,self.d):\n",
      "            mu_k = self.mean_mu[k]\n",
      "            for n in range(0,self.N):\n",
      "                t_n_k = X.T[n][k]\n",
      "                x_n = np.reshape(self.means_z.T[n],(self.q,1))\n",
      "                S += x_n*(t_n_k - mu_k)\n",
      "            self.means_w[k] = np.reshape( E_tau* dot(self.sigma_w,S) , self.q )\n",
      "                    \n",
      "        \n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        self.a_alpha_tilde = self.a_alpha + self.d/2\n",
      "        for i in range(0 , self.q):\n",
      "            self.bs_alpha_tilde[i] = self.b_alpha + (np.trace(np.cov(self.means_w)) +dot(self.means_w.T[i].T,self.means_w.T[i])) / 2\n",
      "         \n",
      "        \n",
      "    def __update_tau(self, X):\n",
      "        self.a_tau_tilde = self.a_tau + self.N*self.d / 2\n",
      "        A = 0\n",
      "        E_W = self.means_w\n",
      "        for i in range(0,self.N):\n",
      "            t_n = np.reshape(X.T[i],(self.d,1))\n",
      "            x_n = np.reshape(self.means_z.T[i],(self.q,1))\n",
      "            A += dot(t_n.T,t_n) + np.trace(self.sigma_mu) + dot(self.mean_mu.T,self.mean_mu)\n",
      "            A += np.trace( dot(dot(E_W.T,E_W), self.sigma_z + dot(x_n,x_n.T)) )\n",
      "            A += 2*dot(dot(self.mean_mu.T,self.means_w),x_n)\n",
      "            A += -2* dot(dot(t_n.T,self.means_w),x_n) - 2*dot(t_n.T,self.mean_mu)\n",
      "        self.b_tau_tilde = self.b_tau + 0.5*A\n",
      "\n",
      "    def L(self, X):\n",
      "        \n",
      "        L = 0\n",
      "        ###Terms from expectations###\n",
      "        #N(X_n|Z_n)\n",
      "        L += -self.N*self.d/2 * (special.digamma(self.a_tau_tilde) - np.log(self.b_tau_tilde)) - self.d/2 *np.log(2*np.pi)\n",
      "        S = 0\n",
      "        for n in range(0,self.N):\n",
      "            S += dot(X[:,n].T, X[:,n])\n",
      "            S += dot(dot(X[:,n].T,self.means_w),self.means_z[:,n]) \n",
      "            S += dot(X[:,n].T, self.mean_mu)[0]\n",
      "            S -= dot(dot(self.means_w.T,self.means_z[:,n].T) ,X[:,n])\n",
      "            S -= dot(dot(self.means_w.T,self.means_z[:,n].T) ,self.mean_mu)    \n",
      "            S += dot( self.mean_mu.T,X[:,n])[0]\n",
      "            S -= dot(dot(self.mean_mu.T,self.means_w) ,self.means_z[:,n]) \n",
      "            S += np.trace(self.sigma_mu) + dot( self.mean_mu.T,self.mean_mu)[0]\n",
      "            S += dot(self.mean_mu.T, X[:,n].T)[0]\n",
      "            S -= dot (dot (self.mean_mu.T, self.means_w), self.means_z[:,n])[0]\n",
      "            S += np.trace(self.sigma_mu) + dot(self.mean_mu.T, self.mean_mu)[0][0]\n",
      "        L +=  self.N/2 *(self.a_tau_tilde/self.b_tau_tilde)*S\n",
      "        \n",
      "        #sum ln N(z_n)\n",
      "        L += - self.N / 2 * np.trace(self.sigma_z) - self.N*self.d/2 *np.log(2*np.pi)\n",
      "        for n in range(0,self.N):\n",
      "            L += - 1/2 * dot(self.means_z.T[n].T, self.means_z.T[n])\n",
      "        \n",
      "        #sum ln(W|a)\n",
      "        L += self.q * self.d /2 * ((special.digamma(self.a_alpha_tilde) - log(self.bs_alpha_tilde[i][0]) ) - np.log(2*np.pi) )\n",
      "        for i in range(0, self.q):\n",
      "            L+= -self.q /2 * (self.a_alpha_tilde / self.bs_alpha_tilde[i][0]) /   (- 1/2) *  (np.trace(self.sigma_w) + dot(self.means_w.T[i].T, self.means_w.T[i]))\n",
      "        \n",
      "        #sum ln (Ga(a_i))\n",
      "        L += self.q * (- log(special.gamma(self.a_alpha)) + self.a_alpha * log(self.b_alpha) )\n",
      "        for i in range(0, self.q):\n",
      "            L +=  -log(self.bs_alpha_tilde[i][0]) + special.digamma(self.a_alpha_tilde)- self.b_alpha * (self.a_alpha_tilde / self.bs_alpha_tilde[i][0])\n",
      "        \n",
      "        #ln(N(\\mu))\n",
      "        L += self.d/2 * (np.log(self.beta) - np.log(2 * np.pi) ) - self.beta/2*(np.trace(self.sigma_mu) + dot(self.mean_mu.T, self.mean_mu)[0][0])\n",
      "        \n",
      "        #ln(Ga(\\tau))\n",
      "        L +=  -log(self.b_tau_tilde[0]) + special.digamma(self.a_tau_tilde) - (self.a_tau_tilde / self.b_tau_tilde[0])\n",
      "\n",
      "        ###Terms from entropies\n",
      "        #H[Q(Z)]\n",
      "        L += self.N*( self.d/2*(1+ np.log(2*np.pi)) + 1/2 * log(linalg.det(self.sigma_z)))\n",
      "        \n",
      "        #H[Q(\\mu)]\n",
      "        L += (0.5)*log(linalg.det(self.sigma_mu))  + self.d/2*(1+ np.log(2*np.pi))\n",
      "        \n",
      "        #H[Q(W)]\n",
      "        L += self.d*(self.d /2 *(1+ np.log(2*np.pi)) + 1/2*log(linalg.det(self.sigma_w)) )\n",
      "        \n",
      "        #H[Q(\\alpha)]\n",
      "        L += self.q * (self.a_alpha_tilde + log(special.gamma(self.a_alpha_tilde)) + (1-self.a_alpha_tilde)*special.digamma(self.a_alpha_tilde)) \n",
      "        for i in range(0,self.d):\n",
      "            L += -log(self.bs_alpha_tilde[i][0])\n",
      "      \n",
      "        #H[Q(\\tau)]\n",
      "        L += self.a_tau_tilde - log(self.b_tau_tilde[0]) + (1-self.a_tau_tilde) * special.digamma(self.a_tau_tilde)\n",
      "        ## the term Gamma(a_tau_tilde) is inf so we ignore it...\n",
      "        \n",
      "        return L\n",
      "  \n",
      "    \n",
      "    def printstuff(self):\n",
      "        print \" ------------- INFO --------------------------\"\n",
      "        print \"Shape of means_z : \" , self.means_z.shape\n",
      "        print \"Shape of sigma_z : \" , self.sigma_z.shape\n",
      "        print \"mean_mu : \" , self.mean_mu\n",
      "        print \"sigma_mu : \" , self.sigma_mu\n",
      "        print \"Shape of means_w : \" , self.means_w.shape\n",
      "        print \"sigma_w : \" , self.sigma_w\n",
      "        print \"a_alpha_tilde : \" , self.a_alpha_tilde \n",
      "        print \"bs_alpha_tilde  : \" , self.bs_alpha_tilde \n",
      "        print \"a_tau_tilde : \" , self.a_tau_tilde\n",
      "        print \"b_tau_tilde : \" , self.b_tau_tilde \n",
      "        pass\n",
      "        \n",
      "    def fit(self, X , iterations = 10 , it = 50):\n",
      "        ## In this implementation we do two layers of iteration , one for each parameter separetely\n",
      "        ## and one for all the parameters.\n",
      "        for i in range(0,iterations):\n",
      "            for j in range(it):\n",
      "                self.__update_z(X)\n",
      "            for j in range(it):\n",
      "                self.__update_mu(X)\n",
      "            for j in range(it):\n",
      "                self.__update_w(X)\n",
      "            for j in range(it):\n",
      "                self.__update_alpha()\n",
      "            for j in range(it):\n",
      "                self.__update_tau(X)\n",
      "            if i %2 == 1 :\n",
      "                print \"------------------------------------\"\n",
      "                print \"Iteration : \" , i\n",
      "                print \"Lower Bound Value : \" , self.L(X)\n",
      "                print \"------------------------------------\"\n",
      "#                 self.printshapes()\n",
      "\n",
      "    def fit2(self, X , iterations = 1000):\n",
      "        ##Simple iterative implementation\n",
      "        import random\n",
      "        for i in range(0,iterations):\n",
      "            self.__update_z(X)\n",
      "            self.__update_mu(X)\n",
      "            self.__update_w(X)\n",
      "            self.__update_alpha()\n",
      "            self.__update_tau(X)\n",
      "                \n",
      "            if i % (iterations/10) == 1 :\n",
      "                print \"------------------------------------\"\n",
      "                print \"Iteration : \" , i\n",
      "                print \"Lower Bound Value : \" , self.L(X)\n",
      "                print \"------------------------------------\"\n",
      "#                 self.printshapes()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that from the equation $\\ln p(D) = \\mathcal{L}(Q) + \\mathrm{KL}(Q||P)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Answer:\n",
      "\n",
      "The distribution $Q(\\Theta)$ is an approximation of the true posterior distribution $p(\\Theta|D)$. From the given equation we can see that the lower bound will reach $\\ln P(D)$ when the KL distance of $Q$ and  $p(\\Theta|D)$ is zero , which is true when $Q$ is equal to  $p(\\Theta|D)$.\n",
      "\n",
      "\n",
      "$$\\mathrm{KL}(Q||P) = - \\int Q(\\Theta) ln \\frac{P(\\Theta|D)}{ Q(\\Theta) } d\\Theta$$ \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###We provide two different solutions , one following the book and one using direct maximisation of $\\mathcal{L}(Q)$ :\n",
      "####Answer (From Bishop)\n",
      "$$\\mathcal{L}(Q) = \\int Q(\\Theta) ln \\frac{p(D,\\Theta)}{Q(\\Theta)} d\\Theta = \\int \\prod_{k}[Q_k(\\theta_k)] \\Bigg( ln[P(D,\\Theta)] - \\sum_{j}[Q_j(\\theta_j)] \\Bigg) d\\Theta = $$\n",
      "\n",
      "$$ = \\int Q_j(\\theta_j) \\Bigg( \\int ln[P(D,\\Theta)] \\prod_{i \\neq j}  Q_i(\\theta_i) d\\theta_i \\Bigg) d\\theta_j - \\int Q_j(\\theta_j) ln[Q_j(\\theta_j)] d\\theta_j + const  =$$\n",
      "\n",
      "\n",
      "$$ = \\int Q_j(\\theta_j)  ln[\\tilde{p}(D,\\Theta_j)] d\\theta_j - \\int Q_j(\\theta_j) ln[Q_j(\\theta_j)] d\\theta_j + const $$\n",
      "\n",
      "where:\n",
      "\n",
      "$$ ln[\\tilde{p}(D,\\Theta_i)] = \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i}  + const$$\n",
      "and \n",
      "$$  \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} = \\int ln[P(D,\\Theta)] \\prod_{i \\neq j}  Q_i(\\theta_i) d\\theta_i $$\n",
      "\n",
      "We can see that the last result of the $\\mathcal{L}(Q)$ calculation is the negative KL divergence between $Q_j(\\theta_j)$ and $\\tilde{p}(D,\\Theta_i)$. In order to maximize $\\mathcal{L}(Q)$ we can try to minimize that KL divergence , which happens when $Q_j(\\theta_j) = \\tilde{p}(D,\\Theta_i)$. This way we get a general expression for the update of $Q_j(\\theta_j)$ which when normalised takes the form:\n",
      "\n",
      "$$Q_i(\\theta_i) = \\frac{\\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ]}{ \\int \\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ] d\\theta_i}$$\n",
      "\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Answer 2 \n",
      "\n",
      "$$\\mathcal{L}(Q) = \\int Q(\\Theta) ln \\frac{p(D,\\Theta)}{Q(\\Theta)} d\\Theta = \\int \\prod_{k}[Q_k(\\theta_k)] ln[P(D,\\Theta)] d\\Theta -  \\int \\prod_{k}[Q_k(\\theta_k)] ln[\\prod_{j}[Q_j(\\theta_j)]] d\\Theta = $$\n",
      "\n",
      "\n",
      "$$ = \\int Q_i(\\theta_i) \\Bigg( \\int\\prod_{k\\neq i}[Q_k(\\theta_k)]ln[P(D,\\Theta)] d\\Theta_{ \\backslash i} \\Bigg) d\\theta_i -  \\int Q_i(\\theta_i) \\Bigg[ \\int \\prod_{k\\neq i}[Q_k(\\theta_k)] \\Bigg( ln[Q_i(\\theta_i)] + \\sum_{j \\neq i} ln[Q_j(\\theta_j)]\\Bigg) d\\Theta_{\\backslash i} \\Bigg]  d\\theta_i  = $$\n",
      "\n",
      "\n",
      "$$ = \\int Q_i(\\theta_i) \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} d\\theta_i -  \\int  Q_i(\\theta_i) ln[Q_i(\\theta_i)]d\\theta_i  \\int \\prod_{k\\neq i}[Q_k(\\theta_k)] d\\Theta_{\\backslash i}  -  \\int  Q_i(\\theta_i) \\Bigg( \\int \\prod_{k\\neq i}[Q_k(\\theta_k)] \\sum_{j \\neq i}[ ln[Q_j(\\theta_j)]] d\\Theta_{\\backslash i} \\Bigg) d\\theta_i $$\n",
      "\n",
      "\n",
      "$$ = \\int Q_i(\\theta_i) C_1  \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} d\\theta_i -  \\int C_1 Q_i(\\theta_i) ln[Q_i(\\theta_i)]d\\theta_i  - \\int  Q_i(\\theta_i) C_1 C_2 d\\theta_i $$\n",
      "\n",
      "\n",
      "$$\\frac{\\partial \\mathcal{L}(Q)}{ \\partial\\theta_i }  = C_1 \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} Q_i(\\theta_i) -  C_1 Q_i(\\theta_i) ln[Q_i(\\theta_i)] - C_1 C_2 Q_i(\\theta_i) = 0 $$ $$ \\rightarrow  \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} - ln[Q_i(\\theta_i)] - C_2 = 0 $$\n",
      "\n",
      "\n",
      "\n",
      "$$\\rightarrow    Q_i(\\theta_i) = C_3 \\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ]$$\n",
      "where \n",
      "$$C_3 = \\frac{1}{ \\int \\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ] d\\theta_i}$$\n",
      "And after normalisation:\n",
      "$$Q_i(\\theta_i) = \\frac{\\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ]}{ \\int \\exp [ \\langle ln[P(D,\\Theta)] \\rangle_{k\\neq i} ] d\\theta_i}$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Answer\n",
      "We cannot use $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$ to track the convergence of the Bayesian PCA algorithm as we are now using distributions to represent the parameters , so we cannot get a fixed point value for them to use in the calculation.\n",
      "The log-prob of the data and parameters are given from:\n",
      "$$\\ln P(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\alpha, \\tau, \\mu) = \\ln\\left(\\prod_{n=1}^N P(x_{n}|z_{n},W,\\mu,\\tau)P(Z)P(W|a)P(a)P(\\mu)P(\\tau)\\right) = $$\n",
      "\n",
      "\n",
      "$$\\ln \\left( \\prod_{n=1}^N \\left[ \\left( \\cfrac{\\tau}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace - \\cfrac{1}{2} \\tau ||x_n - W z_n - \\mu||^2\\right\\rbrace \\right] \\right. \\prod_{n=1}^N \\left[ \\left( \\cfrac{1}{2\\pi}\\right)^{q/2} \\exp \\left\\lbrace -\\cfrac{1}{2} ||z_n||^2 \\right\\rbrace \\right] \\prod_{i=1}^q \\left[ \\left( \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace \\right] \\prod_{i=1}^q \\left[ \\cfrac{b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_i^{\\alpha_{\\alpha - 1}}\\exp \\lbrace -b_{\\alpha}\\alpha_{\\alpha} \\rbrace } {\\Gamma(\\alpha_{\\alpha})} \\right] \\left(\\cfrac{\\beta}{2\\pi}\\right)^{d/2} \\exp \\left\\lbrace -\\cfrac{1}{2}\\beta ||\\mathbf{\\mu}||^2 \\right\\rbrace \\left. \\cfrac{d_{\\tau}^{c_{\\tau}} \\tau^{c_{\\tau} - 1} \\exp \\lbrace -\\tau d_{\\tau} \\rbrace }{\\Gamma(c_{\\tau})} \\right) = $$\n",
      "\n",
      "$$ = \\sum_{n=1}^{N} \\left[ \\ln\\left( \\cfrac{\\tau}{2\\pi}\\right)^{d/2} + ln  \\left( \\cfrac{1}{2\\pi}\\right)^{q/2} + \\left( - \\cfrac{1}{2} \\tau ||x_n - W z_n - \\mu||^2\\right) +  \\left( -\\cfrac{1}{2} ||z_n||^2 \\right)  \\right] + \\sum_{i=1}^{q} \\left[ ln \\left(  \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2}  + \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace + \\ln \\left(  \\left[ \\cfrac{b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_i^{\\alpha_{\\alpha - 1}}\\exp \\lbrace -b_{\\alpha}\\alpha_{\\alpha} \\rbrace } {\\Gamma(\\alpha_{\\alpha})} \\right] \\right) \\right] + \\left( -\\cfrac{1}{2}\\beta ||\\mathbf{\\mu}||^2 \\right) + \\ln \\left( \\cfrac{d_{\\tau}^{c_{\\tau}} \\tau^{c_{\\tau} - 1} \\exp \\lbrace -\\tau d_{\\tau} \\rbrace }{\\Gamma(c_{\\tau})}   \\right) = $$\n",
      "\n",
      "$$ = \\frac{Nd}{2} \\ln\\left( \\cfrac{\\tau}{2\\pi}\\right) + \\frac{Nq}{2} ln \\left( \\cfrac{1}{2\\pi}\\right) + \\sum_{n=1}^{N} \\left[ \\left( - \\cfrac{1}{2} \\tau ||x_n - W z_n - \\mu||^2\\right) +  \\left( -\\cfrac{1}{2} ||z_n||^2 \\right)  \\right] + \\sum_{i=1}^{q} \\left[ ln \\left(  \\cfrac{\\alpha_i}{2\\pi}\\right)^{d/2}  + \\left\\lbrace -\\cfrac{1}{2} \\alpha_i ||\\mathbf{w}_i||^2 \\right\\rbrace + \\ln \\left(  \\left[ \\cfrac{b_{\\alpha}^{\\alpha_{\\alpha}} \\alpha_i^{\\alpha_{\\alpha - 1}}\\exp \\lbrace -b_{\\alpha}\\alpha_{\\alpha} \\rbrace } {\\Gamma(\\alpha_{\\alpha})} \\right] \\right) \\right] + \\left( -\\cfrac{1}{2}\\beta ||\\mathbf{\\mu}||^2 \\right) + \\ln \\left( \\cfrac{d_{\\tau}^{c_{\\tau}} \\tau^{c_{\\tau} - 1} \\exp \\lbrace -\\tau d_{\\tau} \\rbrace }{\\Gamma(c_{\\tau})}   \\right)$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = \\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special).\n",
      "\n",
      "####Answer\n",
      "From equation 10 of [Bishop99], we have that:\n",
      "\\begin{align}\n",
      "\\mathcal{L}(Q) =& \\int Q(\\Theta) \\ln \\dfrac{P(D,\\Theta)}{Q(\\Theta)} d \\Theta\\\\\n",
      "                =& \\int Q(\\Theta) \\ln P(D,\\Theta)  d \\Theta  - \\int Q(\\Theta) \\ln Q(\\Theta)   d \\Theta\n",
      "\\end{align}\n",
      "\n",
      "We name the first term as $E1$ and the second term as $E2$.\n",
      "\n",
      "---\n",
      "\n",
      "We begin with the first term. Using the logarithm product rule, we get:\n",
      "\n",
      "\\begin{align}\n",
      "E1 =& \\int Q(\\Theta) \\ln \\prod_{n=1}^N N(x_n|Wz_n+\\mu, \\tau^{-1}I) d \\Theta \\\\\n",
      "   +& \\int Q(\\Theta) \\ln \\prod_{n=1}^N N(z_n|0, I) d \\Theta \\\\\n",
      "   +& \\int Q(\\Theta) \\ln \\prod_{i=1}^q (\\frac{\\alpha_i}{2\\pi})^{d/2} exp(-\\frac{1}{2} \\alpha_i ||w_i||^2) d \\Theta \\\\\n",
      "   +& \\int Q(\\Theta) \\ln \\prod_{i=1}^q \\Gamma(\\alpha_i|\\alpha_{\\alpha}, b_{\\alpha}) d \\Theta \\\\\n",
      "   +& \\int Q(\\Theta) \\ln N (\\mu|0,\\beta^{-1}I) d \\Theta \\\\\n",
      "   +& \\int Q(\\Theta) \\ln \\Gamma(\\tau|c_{\\tau}, d_{\\tau}) d \\Theta \n",
      "\\end{align}\n",
      "\n",
      "Using the logarithm product rule again, together with the fact that $\\int Q(\\Theta) P(\\Theta_i) d \\Theta =   \\mathbb{E}_{Q(\\Theta_i)} [P(\\Theta_i)] $, where $\\Theta_i$ is the set of variables that $P$ depends on (as the integrals of the other variables will be ones), we write the above as follows :\n",
      "\n",
      "\\begin{align}\n",
      "E1 =& \\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu, \\tau)} \\left[\\ln N(x_n|Wz_n+\\mu, \\tau^{-1}I) \\right] \\\\\n",
      "   +& \\sum_{n=1}^{N} \\mathbb{E}_{Q(z_n)} \\left[\\ln N(z_n|0, I) \\right] \\\\\n",
      "   +& \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i, w_i)} \\left[ \\ln (\\frac{\\alpha_i}{2\\pi})^{d/2} exp(-\\frac{1}{2} \\alpha_i ||w_i||^2)\\right] \\\\\n",
      "   +& \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i)} \\left[ \\ln \\Gamma(\\alpha_i|\\alpha_{\\alpha}, b_{\\alpha}) \\right]\\\\\n",
      "   +& \\mathbb{E}_{Q(\\mu)}\\left[ \\ln N(\\mu|0,\\beta^{-1}I) \\right] \\\\\n",
      "   +& \\mathbb{E}_{Q(\\tau)} \\left[ \\ln \\Gamma(\\tau|c_{\\tau}, d_{\\tau}) \\right]\n",
      "\\end{align}\n",
      "\n",
      "We now derive each term of $E1$ in turn, using the appropriate definition of each distribution in the log-space:\n",
      "\n",
      "####(1)\n",
      "\n",
      "\\begin{align}\n",
      "\\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu, \\tau)} \\left[\\ln N(x_n|Wz_n+\\mu, \\tau^{-1}I) \\right] \n",
      "=& \\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu, \\tau)} \\left[ -\\frac{1}{2} \\ln | \\tau ^{-1}I| - \\frac{d}{2} \\ln 2\\pi - \\frac{1}{2} ((x_n-Wz_n+\\mu)^T \\tau I (x_n-Wz_n+\\mu) \\right] \\\\\n",
      "=& - \\frac{Nd}{2} \\mathbb{E}_{Q(\\tau)} \\left[ \\ln (\\tau)  \\right] - \\frac{Nd}{2} \\ln 2\\pi - \\frac{N}{2} \\mathbb{E}_{Q(\\tau)} \\left[ \\tau \\right] \\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu)} \\left[  (x_n-Wz_n+\\mu)^T (x_n-Wz_n+\\mu) \\right] \\\\ =& - \\frac{Nd}{2} (\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} ) - \\frac{d}{2} \\ln 2\\pi -\\frac{N}{2} \\frac{\\tilde{a}_\\tau}{\\tilde{b}_{\\tau}} * V\n",
      "\\end{align}\n",
      ", where we have used for $\\alpha_i \\sim \\Gamma(a,b)$ and $Q(\\alpha_i) \\sim \\Gamma(\\tilde{a}_\\alpha,\\tilde{b}_{\\alpha i})$,  $\\mathbb{E}_{Q(\\alpha_i)} [\\ln \\alpha_i] = - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha)$ and that $\\mathbb{E}_{Q(\\alpha_i)}[ x] = \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}}$\n",
      "\n",
      "We now solve the remaining part :\n",
      "\\begin{align}\n",
      "    V =& \\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu)} \\left[  (x_n-Wz_n+\\mu)^T (x_n-Wz_n+\\mu) \\right] \\\\\n",
      "      =& \\sum_{n=1}^{N} \\left( \\int \\int \\int Q(z_n) Q(W) Q(\\mu) dz_n d_W d_\\mu \\left[ {x_n}^Tx_n  - {x_n}^TWz_n + {x_n}^T\\mu - W^T{z_n}^Tx_n + W^T{z_n}^TW{z_n} - W^T {z_n}^T \\mu + \\mu^Tx_n - \\mu^T W z_n + \\mu^T\\mu  \\right] \\right) \n",
      "\\end{align}\n",
      "At this point, we split the integral for each term , and we integrate only over its dependent variables, as the other ones will be ones. We also rearrange the variables inside the integrals, so can take the corresponding expectations which we can calculate. We demonstrate this using the second term as an example (the other ones are derived in a similar way) :\n",
      "\\begin{align}\n",
      "    {x_n}^T \\int \\int W Q(W) d W z_n Q(z_n) d z_n  =& {x_n}^T \\int \\mathbb{E}_{Q(W)} \\left[W  \\right] z_n Q(z_n) d z_n \\\\\n",
      "    =& {x_n}^T  \\mathbb{E}_{Q(W)} \\left[W  \\right] \\int   z_n Q(z_n) d z_n \\\\\n",
      "    =& {x_n}^T m_w  \\mathbb{E}_{Q(z_n)} \\left[z_n  \\right] \\\\\n",
      "    =& {x_n}^T m_w {m_z}^{(n)} \n",
      "\\end{align}\n",
      "\n",
      "Going back to the calculation of V :\n",
      "\n",
      "\\begin{align}\n",
      "    V =& \\sum_{n=1}^{N} \\left( \\int \\int \\int Q(z_n) Q(W) Q(\\mu) dz_n d_W d_\\mu \\left[ {x_n}^Tx_n  - {x_n}^TWz_n + {x_n}^T\\mu - W^T{z_n}^Tx_n + W^T{z_n}^TW{z_n} - W^T {z_n}^T \\mu + \\mu^Tx_n - \\mu^T W z_n + \\mu^T\\mu  \\right] \\right) \\\\\n",
      "      =& \\sum_{n=1}^{N} \\left( {x_n}^Tx_n - {x_n}^T m_w {m_z}^{(n)} + {x_n}^Tm_\\mu - {m_w}^T{{m_z}^{(n)}}^Tx_n - {m_w}^T {{m_z}^{(n)}}^T m_\\mu + {m_\\mu}^T x_n - {m_\\mu}^Tm_w {m_z}^{(n)} + Tr(\\Sigma_\\mu) + {m_\\mu}^Tm_\\mu \\right)\n",
      "\\end{align}\n",
      ", where we have used eq.378 from matrix cookbook, and we ignored the term $W^T{z_n}^TW{z_n}$ which we were not able to derive.\n",
      "\n",
      "Putting all together :\n",
      "\\begin{align}\n",
      "\\sum_{n=1}^{N} \\mathbb{E}_{Q(W, z_n, \\mu, \\tau)} \\left[\\ln N(x_n|Wz_n+\\mu, \\tau^{-1}I) \\right] \n",
      "=& - \\frac{Nd}{2} (\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} ) - \\frac{d}{2} \\ln 2\\pi -\\frac{N}{2} \\frac{\\tilde{a}_\\tau}{\\tilde{b}_{\\tau}} \\sum_{n=1}^{N} \\left( {x_n}^Tx_n - {x_n}^T m_w {m_z}^{(n)} + {x_n}^Tm_\\mu - {m_w}^T{{m_z}^{(n)}}^Tx_n - {m_w}^T {{m_z}^{(n)}}^T m_\\mu + {m_\\mu}^T x_n - {m_\\mu}^Tm_w {m_z}^{(n)} + Tr(\\Sigma_\\mu) + {m_\\mu}^Tm_\\mu \\right)\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "---\n",
      "####(2)\n",
      "\\begin{align}\n",
      "\\sum_{n=1}^{N} \\mathbb{E}_{Q(z_n)} \\left[\\ln N(z_n|0, I) \\right] =\n",
      "\\sum_{n=1}^{N} \\mathbb{E}_{Q(z_n)} \\left[-\\frac{1}{2} \\ln |I| - \\frac{d}{2} \\ln 2\\pi -\\frac{1}{2} z_n^Tz_n  \\right] =\n",
      "\\sum_{n=1}^{N} - \\frac{d}{2} \\ln 2\\pi  -\\frac{1}{2} (Tr(\\Sigma_z) + {m_z^{(n)}}^{T}{m_z^{(n)}}) =\n",
      "-\\frac{N}{2}Tr(\\Sigma_z) -\\frac{Nd}{2} \\ln 2\\pi - \\frac{1}{2} \\sum_{n=1}^{N} ((Tr(\\Sigma_z) + {m_z^{(n)}}^{T}{m_z^{(n)}})\n",
      "\\end{align}\n",
      ", where we have used eq.378 from Matrix cookbook\n",
      "\n",
      "---\n",
      "####(3)\n",
      "\\begin{align}\n",
      " \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i, w_i)} \\left[ \\ln (\\frac{\\alpha_i}{2\\pi})^{d/2} exp(-\\frac{1}{2} \\alpha_i ||w_i||^2)\\right] =& \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i, w_i)} \\left[ \\frac{d}{2} \\ln \\alpha_i - \\frac{d}{2} \\ln 2\\pi - \\frac{1}{2} (\\alpha_i||w_i||^2) \\right] \\\\\n",
      " =& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i)} \\left[ \\ln \\alpha_i \\right] + \\mathbb{E}_{Q(\\alpha_i, w_i)} \\left[  (\\alpha_i{w_i}^T{w_i}) \\right] \\\\\n",
      " =& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\left[ - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) + \\int \\int \\alpha_i Q(\\alpha_i) d \\alpha_i {w_i}^T{w_i} Q(w_i) d w_i \\right] \\\\\n",
      " =& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\left(  - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) + \\mathbb{E}_{Q(\\alpha_i)} \\left[ \\alpha_i \\right] \\int {w_i}^T{w_i} Q(w_i) d w_i \\right) \\\\\n",
      " =& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\left(  - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) + \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}} \\mathbb{E}_{Q(w_i)} \\left[ {w_i}^T{w_i}  \\right] \\right) \\\\\n",
      "  =& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\left( - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) + \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}} (Tr(\\Sigma_w) + {{m_w}^{(i)}}^{T} {{m_w}^{(i)}}) \\right)\n",
      "\\end{align}\n",
      ", where we have used for $\\alpha_i \\sim \\Gamma(a,b)$ and $Q(\\alpha_i) \\sim \\Gamma(\\tilde{a}_\\alpha,\\tilde{b}_{\\alpha i})$,   $\\mathbb{E}_{Q(\\alpha_i)}[ x] = \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}}$, $\\mathbb{E}_{Q(\\alpha_i)} [\\ln \\alpha_i] = - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha)$ and eq.378 from the Matrix cookbook\n",
      "\n",
      "---\n",
      "####(4)\n",
      "\\begin{align}\n",
      "\\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i)} \\left[ \\ln \\Gamma(\\alpha_i|\\alpha_{\\alpha}, b_{\\alpha}) \\right] =\n",
      "\\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i)} \\left[ -\\ln \\Gamma(\\alpha_{\\alpha}) +  \\alpha_{\\alpha} \\ln b_{\\alpha} + (\\alpha_{\\alpha}-1) \\ln \\alpha_i -  b_{\\alpha} \\alpha_i   \\right] \\\\ &=\n",
      "q (-\\ln \\Gamma(\\alpha_{\\alpha}) +  \\alpha_{\\alpha} \\ln b_{\\alpha}) + \\sum_{i=1}^{q} \\mathbb{E}_{Q(\\alpha_i)} \\left[  (\\alpha_{\\alpha}-1) \\ln \\alpha_i -  b_{\\alpha} \\alpha_i   \\right] \\\\ &=\n",
      "q (-\\ln \\Gamma(\\alpha_{\\alpha}) +  \\alpha_{\\alpha} \\ln b_{\\alpha}) + \\sum_{i=1}^{q} \\left[ - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) - b_\\alpha \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}} \\right]\n",
      "\\end{align}\n",
      "\n",
      ", where we have used for $\\alpha_i \\sim \\Gamma(a,b)$ and $Q(\\alpha_i) \\sim \\Gamma(\\tilde{a}_\\alpha,\\tilde{b}_{\\alpha i})$,  $\\mathbb{E}_{Q(\\alpha_i)} [\\ln \\alpha_i] = - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha)$ and that $\\mathbb{E}_{Q(\\alpha_i)}[ x] = \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}}$\n",
      "\n",
      "---\n",
      "####(5)\n",
      "\\begin{align}\n",
      "\\mathbb{E}_{Q(\\mu)}\\left[ \\ln N(\\mu|0,\\beta^{-1}I) \\right] = \n",
      "\\frac{d}{2} (\\ln(\\beta) - \\ln(2\\pi)) - \\frac{\\beta}{2} (Tr(\\Sigma_\\mu) + m_\\mu^{T}m_\\mu)\n",
      "\\end{align},\n",
      "where we have used the same derivation as in the second term of E1.\n",
      "\n",
      "---\n",
      "####(6)\n",
      "\\begin{align}\n",
      "\\mathbb{E}_{Q(\\tau)} \\left[ \\ln \\Gamma(\\tau|c_{\\tau}, d_{\\tau}) \\right] = c_\\tau \\ln (d_\\tau) - \\ln \\Gamma(c_\\tau) + (c_\\tau-1) (\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} ) - d_\\tau \\frac{\\tilde{a}_\\tau}{\\tilde{b}_{\\tau}}\n",
      "\\end{align}\n",
      ", where we have used the same derivation as in the fourth term of E1.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "Using the logarithm product rule, and the fact that $-\\int Q(\\Theta) \\ln Q(\\Theta) = H[Q(\\Theta)] $, where $H[Q(\\Theta)] $ is the Shannon Entropy, we get:\n",
      "\\begin{align}\n",
      "    E2 =& \\sum_{n=1}^{N} H\\left[ N(z_n|m_x^{(n)}, \\Sigma_x)\\right] \\\\\n",
      "        +& H\\left[ N(\\mu| m_\\mu, \\Sigma_\\mu) \\right] \\\\\n",
      "        +& \\sum_{k=1}^{d} H\\left[ N(\\tilde{w}_k | m_w^{(k)}, \\Sigma_w )  \\right] \\\\\n",
      "        +& \\sum_{i=1}^{q} H\\left[ \\Gamma(\\tilde{a}_i|\\alpha_{\\alpha}, \\tilde{b}_{\\alpha i}) \\right] \\\\\n",
      "        +& H\\left[ \\Gamma(\\tau|\\tilde{a}_{\\tau}, \\tilde{b}_{\\tau}) \\right] \n",
      "\\end{align}\n",
      "Using the standard definitions of entropy (from Wikipedia) we get:\n",
      "\\begin{align}\n",
      "    E2 =& N (\\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{z}|)  \\\\\n",
      "       +& \\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{\\mu}| \\\\\n",
      "       +& d (\\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{w}|) \\\\\n",
      "       +& q (\\tilde{a}_\\alpha  + ln \\Gamma(\\tilde{a}_\\alpha) + (1-\\tilde{a}_a) \\psi(\\tilde{a}_\\alpha)) -  \\sum_{i}^{q} \\ln  \\tilde{b}_{\\alpha i} \\\\\n",
      "       +& \\tilde{a}_\\tau - \\ln \\tilde{b}_{\\tau} + ln \\Gamma(\\tilde{a}_\\tau) + (1-\\tilde{a}_\\tau) \\psi(\\tilde{a}_\\tau)\n",
      "\\end{align}\n",
      "\n",
      "---\n",
      "\n",
      "Combining E1 and E2, we finally get :\n",
      "\\begin{align}   \n",
      "  \\mathcal{L}(Q)   =& - \\frac{Nd}{2} (\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} ) - \\frac{d}{2} \\ln 2\\pi -\\frac{N}{2} \\frac{\\tilde{a}_\\tau}{\\tilde{b}_{\\tau}} \\sum_{n=1}^{N} \\left( {x_n}^Tx_n - {x_n}^T m_w {m_z}^{(n)} + {x_n}^Tm_\\mu - {m_w}^T{{m_z}^{(n)}}^Tx_n - {m_w}^T {{m_z}^{(n)}}^T m_\\mu + {m_\\mu}^T x_n - {m_\\mu}^Tm_w {m_z}^{(n)} + Tr(\\Sigma_\\mu) + {m_\\mu}^Tm_\\mu \\right) \\\\\n",
      "                   -& \\frac{N}{2}Tr(\\Sigma_z) -\\frac{Nd}{2} \\ln 2\\pi - \\frac{1}{2} \\sum_{n=1}^{N} ((Tr(\\Sigma_z) + {m_z^{(n)}}^{T}{m_z^{(n)}}) \\\\\n",
      "                   +& \\frac{qd}{2}(- \\ln 2\\pi) - \\frac{1}{2} \\sum_{i=1}^{q} \\left( - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) + \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}} (Tr(\\Sigma_w) + {{m_w}^{(i)}}^{T} {{m_w}^{(i)}}) \\right) \\\\\n",
      "                   +& q (-\\ln \\Gamma(\\alpha_{\\alpha}) +  \\alpha_{\\alpha} \\ln b_{\\alpha}) + \\sum_{i=1}^{q} \\left[ - \\ln \\tilde{b}_{\\alpha i} + \\psi(\\tilde{a}_\\alpha) - b_\\alpha \\frac{\\tilde{a}_\\alpha}{\\tilde{b}_{\\alpha i}} \\right] \\\\\n",
      "                   +& \\frac{d}{2} (\\ln(\\beta) - \\ln(2\\pi)) - \\frac{\\beta}{2} (Tr(\\Sigma_\\mu) + m_\\mu^{T}m_\\mu) \\\\\n",
      "                   +& c_\\tau \\ln (d_\\tau) - \\ln \\Gamma(c_\\tau) + (c_\\tau-1) (\\psi(\\tilde{a}_\\tau) - \\ln \\tilde{b}_{\\tau} ) - d_\\tau \\frac{\\tilde{a}_\\tau}{\\tilde{b}_{\\tau}} \\\\\n",
      "                   +& N (\\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{z}|)  \\\\\n",
      "                   +& \\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{\\mu}| \\\\\n",
      "                   +& d (\\frac{d}{2}(1+\\ln 2\\pi) + \\frac{1}{2} \\ln |\\Sigma_{w}|) \\\\\n",
      "                   +& q (\\tilde{a}_\\alpha  + ln \\Gamma(\\tilde{a}_\\alpha) + (1-\\tilde{a}_a) \\psi(\\tilde{a}_\\alpha)) -  \\sum_{i}^{q} \\ln  \\tilde{b}_{\\alpha i} \\\\\n",
      "                   +& \\tilde{a}_\\tau - \\ln \\tilde{b}_{\\tau} + ln \\Gamma(\\tilde{a}_\\tau) + (1-\\tilde{a}_\\tau) \\psi(\\tilde{a}_\\tau)\n",
      "\\end{align}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 5. Optimize variational parameters (50 points)\n",
      "Implement the update equations for the Q-distributions, in the __update_XXX methods. Each update function should re-estimate the variational parameters of the Q-distribution corresponding to one group of variables (i.e. either $Z$, $\\mu$, $W$, $\\alpha$ or $\\tau$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6. Learning algorithm (10 points)\n",
      "Implement the learning algorithm described in [Bishop99], i.e. iteratively optimize each of the Q-distributions holding the others fixed.\n",
      "\n",
      "What would be a good way to track convergence of the algorithm? Implement your suggestion.\n",
      "\n",
      "Test the algorithm on some test data drawn from a Gaussian with different variances in orthogonal directions. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Data Generation using numpy multivariate random , We use the same values described by [Bishop99].\n",
      "X = np.random.multivariate_normal(np.zeros(10), np.diag([5,4,3,2,1,1,1,1,1,1]), 100).T\n",
      "\n",
      "##Run Algorithm\n",
      "test = BayesianPCA(10,100)\n",
      "test.fit2(X,50) \n",
      "hinton(test.means_w )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnboundLocalError",
       "evalue": "local variable 'i' referenced before assignment",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-a77302774579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##Run Algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mhinton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_w\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-e1138bfdc12f>\u001b[0m in \u001b[0;36mfit2\u001b[0;34m(self, X, iterations)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"------------------------------------\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Iteration : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Lower Bound Value : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"------------------------------------\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;31m#                 self.printshapes()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-6-e1138bfdc12f>\u001b[0m in \u001b[0;36mL\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m#sum ln(W|a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_alpha_tilde\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_alpha_tilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mL\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_alpha_tilde\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs_alpha_tilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "------------------------------------\n",
        "Iteration :  1\n",
        "Lower Bound Value : "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##The following is the implemetation we found for viewing the hinton diagram of the resulting W matrix.\n",
      "import pylab as pl\n",
      "\n",
      "def _blob(x,y,area,colour):\n",
      "    \"\"\"\n",
      "    Source: http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams\n",
      "    Draws a square-shaped blob with the given area (< 1) at\n",
      "    the given coordinates.\n",
      "    \"\"\"\n",
      "    hs = np.sqrt(area) / 2\n",
      "    xcorners = np.array([x - hs, x + hs, x + hs, x - hs])\n",
      "    ycorners = np.array([y - hs, y - hs, y + hs, y + hs])\n",
      "    pl.fill(xcorners, ycorners, colour, edgecolor=colour)\n",
      "    \n",
      "def hinton(W, maxWeight=None):\n",
      "    \"\"\"\n",
      "    Source: http://wiki.scipy.org/Cookbook/Matplotlib/HintonDiagrams\n",
      "    Draws a Hinton diagram for visualizing a weight matrix.\n",
      "    Temporarily disables matplotlib interactive mode if it is on,\n",
      "    otherwise this takes forever.\n",
      "    \"\"\"\n",
      "    reenable = False\n",
      "    if pl.isinteractive():\n",
      "        pl.ioff()\n",
      "    pl.clf()\n",
      "    height, width = W.shape\n",
      "    if not maxWeight:\n",
      "        maxWeight = 2**np.ceil(np.log(np.max(np.abs(W)))/np.log(2))\n",
      "\n",
      "    pl.fill(np.array([0,width,width,0]),np.array([0,0,height,height]),'gray')\n",
      "    pl.axis('off')\n",
      "    pl.axis('equal')\n",
      "    for x in xrange(width):\n",
      "        for y in xrange(height):\n",
      "            _x = x+1\n",
      "            _y = y+1\n",
      "            w = W[y,x]\n",
      "            if w > 0:\n",
      "                _blob(_x - 0.5, height - _y + 0.5, min(1,w/maxWeight),'white')\n",
      "            elif w < 0:\n",
      "                _blob(_x - 0.5, height - _y + 0.5, min(1,-w/maxWeight),'black')\n",
      "    if reenable:\n",
      "        pl.ion()\n",
      "    pl.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 7. PCA Representation of MNIST (10 points)\n",
      "\n",
      "Download the MNIST dataset from here http://deeplearning.net/tutorial/gettingstarted.html (the page contains python code for loading the data). Run your algorithm on (part of) this dataset, and visualize the results.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle, gzip, numpy\n",
      "\n",
      "# Load the dataset\n",
      "f = gzip.open('mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}